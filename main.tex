\documentclass[11pt]{book}

\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{url}
\usepackage{amssymb,amsthm}
\usepackage[utf8]{inputenc}
%\usepackage{inconsolata}
\usepackage{sourcecodepro}

\input{futhark.tex}

\usepackage{color}
\definecolor{eclipseBlue}{RGB}{42,0.0,255}
\newcommand{\soac}[1]{\texttt{\color{eclipseBlue}#1}}

\title{\bf Parallel Programming in Futhark}
\author{HIPERFIT \\ Department of Computer Science \\ University of Copenhagen (DIKU)}
\date{\today}

\begin{document}
\frontmatter
\maketitle
\chapter{Preface}

These notes ...

\tableofcontents
\mainmatter
\part{Parallel Functional Programming}
\chapter{Introduction}

\begin{enumerate}
\item Moores law, CPUs, GPUs, other parallel architectures
\item Concurrency vs parallelism
\item Task parallelism, data parallism, simd, mimd
\item Low-level languages vs high-level language approaches
\end{enumerate}

See \cite{finpar}.

\chapter{The Futhark Language}

Futhark is a pure functional data-parallel array language.  Is is both
syntactically and conceptually similar to established functional
languages, such as Haskell or Standard ML.  In contrast to these
languages, Futhark focuses less on expressivity and elaborate type
systems, but more on compilation to high-performance parallel code.
Futhark comes with language constructs for performing bulk operations
on arrays, called \textit{Second-Order Array Combinators} (SOACs),
that mirror the higher order functions found in conventional
functional languages: \texttt{map}, \texttt{reduce}, \texttt{filter},
and so forth.  In Futhark, SOACs are not merely library functions, but
built-in language features with parallel semantics, and which will
typically be compiled to parallel code.

The primary idea behind Futhark is to design a language that has
enough expressive power to conveniently express complex programs, yet
is also amenable to aggressive optimisation and parallelisation.  The
tension is that as the expressive power of a language grows, the
difficulty of efficient compilation rises likewise.  For example,
Futhark supports nested parallelism, despite the complexities of
efficiently mapping it to the flat parallelism supported by hardware,
as many algorithms are awkward to write with just flat parallelism.
On the other hand, we do not support non-regular arrays, as they
complicate size analysis a great deal.  The fact that Futhark is
purely functional is intended to give an optimising compiler more
leeway in rearranging the code and performing high-level
optimisations.

Programming in Futhark feels similar to programming in other
functional languages.  If you know Haskell or Standard ML, you will
likely be able to read and modify most Futhark code.  For example,
this program computes the dot product $\Sigma_{i} x_{i}\cdot{}y_{i}$
of two vectors of integers:

\lstinputlisting{src/dotprod.fut}

In Futhark, the notation for an array of element type $t$ is
\texttt{[]$t$}.  The program declares a function called \texttt{main}
that takes two arguments, both integer arrays, and returns an integer.
The program first computes computes the element-wise product of its
two arguments, resulting in an array of integers, then computes the
product of the elements in this new array.

If you put this program in a file \texttt{dotprod.fut}, then you can
compile it to a binary \texttt{dotprod} (or \texttt{dotprod.exe} on
Windows) by running:

\begin{verbatim}
$ futhark-c dotprod.fut
\end{verbatim}

A Futhark program compiled to an executable will read the arguments to
its \texttt{main} function from standard input, and will print the
result to standard output:

\begin{verbatim}
$ echo [2,2,3] [4,5,6] | ./dotprod
36i32
\end{verbatim}

In Futhark, an array literal is written with square brackets
surrounding a comma-separated sequence of elements.  Integer literals
can be suffixed with a specific type.  This is why \texttt{dotprod}
prints \texttt{36i32}, rather than just \texttt{36} - this makes it
clear that the result is a 32-bit integer.  Later we will see when
these suffixes can be useful.

The \texttt{futhark-c} compiler we used above translates a Futhark
program into sequential code running on the GPU.  This can be useful
for testing, and will work on most systems, even those without GPUs.
However, this wastes the main potential of Futhark: fast parallel
execution.  We can instead use the \texttt{futhark-opencl} compiler to
generate an executable that offloads execution via the OpenCL
framework.  In principle, this allows offloading to any kind of
device, but the \texttt{futhark-opencl} compilation pipelines makes
optimisation assumptions that are oriented towards contemporary GPUs.
Use of \texttt{futhark-opencl} is simple, assuming your system has a
working OpenCL setup:

\begin{verbatim}
$ futhark-opencl dotprod.fut
\end{verbatim}

And execution is just as before:

\begin{verbatim}
$ echo [2,2,3] [4,5,6] | ./dotprod
36i32
\end{verbatim}

In this case, the workload is small enough that there is little
benefit in parallelising the execution.  In fact, it is likely that
the OpenCL startup overhead results in several orders of magnitude
slowdown, over sequential execution, for this tiny dataset.  See
Section~\ref{sec:benchmarking} for information on how to measure
execution times.

The ability to compile Futhark programs to executables is useful for
testing, but it should be noted that it is not how Futhark is intended
to be used in practice.  As a pure functional array language, Futhark
is not capable of input handling or user interfaces, and as such
cannot be used as a general-purpose language.  Futhark is intended to
be used for small, performance-sensitive parts of larger applications,
typically by compiling a Futhark program to a \textit{library} that
can be imported and used by applications written in conventional
languages.  While this usage is still work-in-progress, see
Chapter~\ref{chap:interoperability} for more information.

As befits their use for testing, compiled Futhark executables take a
range of command line options to manipulate their behaviour and print
debugging information.  These will be introduced as neded.

\section{Basic Language Features}

As a functional or \textit{value-oriented} language, Futhark can be
understood entirely by how values are constructed, and how expressions
transform one value to another.  As a statically typed language, all
values are classified by their \textit{type}.  The primitive types in
Futhark are the signed integer types \texttt{i8}, \texttt{i16},
\texttt{i32}, \texttt{i64}, the unsigned integer types \texttt{u8},
\texttt{u16}, \texttt{u32}, \texttt{u64}, the floating-point types
\texttt{f32}, \texttt{f64}, as well as \texttt{bool}.  Furthermore,
\texttt{int} is an alias for \texttt{i32}.  An \texttt{f32} is always
a single-precision float and a \texttt{f64} is a double-precision
float.

Numeric literals can be suffixed with their intended type.  For
example \texttt{42i8} is of type \texttt{i8}, and \texttt{1337e2f64}
is of type \texttt{f64}.  If no suffix is given, integer literals are
of type \texttt{i32}, and decimal literals are of type \texttt{f64}.
Boolean literals are written as \texttt{true} and \texttt{false}.

All primitive values can be combined in tuples and arrays.  A tuple
value or type is written as a sequence of comma-separated values or
types enclosed in parentheses.  For example, \texttt{(0, 1)} is a
tuple value of type \texttt{(i32,i32)}.  The elements of a tuple need
not have the same type -- the value \texttt{(False, 1, 2.0)} is of
type \texttt{(bool, i32, f64)}.  A tuple element can also be another
tuple, as in \texttt{((1,2),(3,4))}, which is of type
\texttt{((i32,i32),(i32,i32))}.  A tuple cannot have just one element,
but empty tuples are permitted (if not very useful) -- these are
written as simply \texttt{()} and are of type \texttt{()}.

An array value is written as a nonempty sequence of comma-separated
values enclosed in square brackets.  An array type is written as
\texttt{[]$t$}, where \texttt{$t$} is the element type of the array.
For example, an array of three integers could be written as
\texttt{[1,2,3]}, and has type \texttt{[]i32}.  An empty array is
written as \texttt{empty($t$)}, where \texttt{$t$} is the element
type.  Multi-dimensional arrays are supported in Futhark, but they
must be \textit{regular}.  This means that all inner arrays must have
the same shape.  For example, \texttt{[[1,2], [3,4]]} is a valid array
of type \texttt{[][]i32}, but \texttt{[[1,2], [3,4,5]]} is not.  We
will return to the implications of this restriction in later chapters.

\subsection{Simple Expressions}

The Futhark expression syntax is mostly conventional, and supports the
usual binary and unary operators, with few surprises.  This section
will not try to cover the entire Futhark expression language in
complete detail.  See the reference manual at
\url{http://futhark.readthedocs.io} for a comprehensive treatment.

Function calls are written using simple juxtaposition.  For example,
to apply the predefined \texttt{cos64} function (cosinus for
\texttt{f64} values) to a constant argument, we write:

\begin{lstlisting}
cos64 1.0
\end{lstlisting}

See Section \ref{sec:function-declarations} for how to declare your
own functions.

A \texttt{let}-expression can be used to give a name to the result of
an expression:

\begin{lstlisting}
  let z = x + y
  in body
\end{lstlisting}

Futhark is eagerly evaluated (unlike Haskell), so the expression for
\texttt{z} will be fully evaluated before \texttt{body}.  The \texttt{in} keyword is optional when it precedes another
\texttt{let}.  This means that instead of writing:

\begin{lstlisting}
  let a = 0 in
  let b = 1 in
  let c = 2 in
  a + b + c
\end{lstlisting}

we can write:

\begin{lstlisting}
  let a = 0
  let b = 1
  let c = 2
  in a + b + c
\end{lstlisting}

The final \texttt{in} is still necessary.  In examples, we will often
skip the body of a \texttt{let} if it is not important.  A limited
amount of pattern matching is supported in \texttt{let}-bindings,
which permits tuple components to be extracted:

\begin{lstlisting}
  let (x,y) = e -- must be of some type (t1,t2)
\end{lstlisting}

This also demonstrates the Futhark line comment syntax - two dashes
followed by a space.  Block comments are not presently supported.

Two-way \texttt{if-then-else} is the only branching construct in
Futhark:

\begin{lstlisting}
if x < 0 then -x else x
\end{lstlisting}

Arrays are indexed using the common row-major notation, e.g.,
\texttt{a[i1, i2, i3...]}.  An indexing is said to be \textit{full} if
the number of given indices is equal to the dimensionality of the
array.  All array accesses are checked at runtime, and the program
will terminate abnormally if an invalid access is attempted.

Indexing binds very tightly.  For example, the expression
\texttt{a~b~[i]} means ``apply the function \texttt{a} to the
expression \texttt{b[i]}'', \textit{not} ``apply the function
\texttt{a} to the expressions \texttt{b} and \texttt{[i]}''.  When the
latter is desired, enclose the literal array with parentheses, as
\texttt{a~b~([i])}.

Futhark also supports array \textit{slices}.  The expression
\texttt{a[i:j]} returns a slice of the array \texttt{a} from index
\texttt{i} to \texttt{j}, the latter inclusive and the latter
exclusive.  Slicing of multiple dimensions can be done by separating
with commas, and may be intermixed freely with indexing.  It is an
error if \texttt{j < n}.

\subsection{Function Definitions}
\label{sec:function-declarations}

A Futhark program consists of a sequence of \textit{function
  definitions}, of the following form:

\begin{lstlisting}
   fun name params... : return_type = body
\end{lstlisting}

A function must declare both its return type and the types of all its
parameters.  All functions (except for inline anonymous functions, as
we will get to later) are defined globally.  Futhark does not use type
inference.  As a concrete example, here is the recursive definition of
the factorial function in Futhark:

\begin{lstlisting}
  fun fact(n: int): int =
    if n == 0 then 1
              else n * fact(n-1)
\end{lstlisting}

Indentation has no syntactical significance in Futhark, but is
recommended for readability.

\section{Array Operations}

Futhark provides various combinators for performing bulk
transformations of arrays.  Judicious use of these combinators is key
to getting good performance.  There are two overall categories:
\textit{first-order array combinators}, like \texttt{concat}, that
always perform the same operation, and \textit{second-order array
  combinators} (\textit{SOAC}s), like \texttt{map}, that take a
\textit{functional argument} indicating the operation to perform.
SOACs are absolutely crucial to Futhark programming.  While they are
designed to resemble higher-order functions that may be familiar from
functional languages, they have implicitly parallel semantics, and
some restrictions to preserve those semantics.

The simplest SOAC is probably \texttt{map}.  It takes two arguments: a
function and an array.  The function argument can be a function name,
or an anonymous function using \texttt{fn} syntax.  The function is
called for every element of input array, and an array of the result is
returned.  For example:

\begin{lstlisting}
map (fn x => x + 2) [1,2,3] == [3,4,5]
\end{lstlisting}

Functions defined using \texttt{fn} syntax need not define their
parameter- or return types, but you are free to do so to increase
readability:

\begin{lstlisting}
map (fn (x:int): int => x + 2) [1,2,3]
\end{lstlisting}

The functional argument can also be an operator, which must be
enclosed in parentheses:

\begin{lstlisting}
map (!) [True, False, True] == [False, True, False]
\end{lstlisting}

Currying for operators is also supported using a syntax taken from
Haskell:

\begin{lstlisting}
map (2-) [1,2,3] == [-1,0,1]
\end{lstlisting}

While \texttt{map} accepts only a single array argument, there is a
variation called \texttt{zipWith}, that takes any nonzero number of
array arguments, and requires a function with the same number of
parameters.  For example, we can perform an element-wise sum of two
arrays:

\begin{lstlisting}
zipWith (+) [1,2,3] [4,5,6] == [5,7,9]
\end{lstlisting}

While \texttt{map} is an array transformer, the \texttt{reduce} SOAC
is an array aggregator: it uses some function of type \texttt{t -> t
  -> t} to combine the elements of an array of type \texttt{[]t} to a
value of type \texttt{t}.  In order to do this in parallel, the
function must be \textit{associative} and have a \textit{neutral
  element} (i.e, form a monoid).

\begin{itemize}
\item A function $f$ is associative if $f(x,f(y,z)) = f(f(x,y),z)$ for
  all $x,y,z$.
\item A function $f$ has a neutral element $e$ if
  $f(x,e) = f(e,x) = x$ for all $x$.
\end{itemize}

Many common mathematical operators fulfill these laws, e.g addition:
$(x+y)+z=x+(y+z)$ and $x+0=0+x=0$.  But others, like subtraction, do
not.  In Futhark, we can use the addition operator and its neutral
element to compute the sum of an array of integers:

\begin{lstlisting}
reduce (+) 0 [1,2,3] == 6
\end{lstlisting}

It turns out that combining \texttt{map} and \texttt{reduce} is both
powerful and has remarkable optimisation properties, as we will
discuss in chapter~\ref{chap:soac-algebra}.  Many Futhark programs are
primarly \texttt{map}-\texttt{reduce} compositions.  For example, we
can define a function to compute the dot product of two vectors of
integers:

\begin{lstlisting}
fun dotProd (xs: []int) (ys: []int): []int =
  reduce (+) 0 (zipWith (+) xs ys)
\end{lstlisting}

\section{Sequential Loops}
\label{sec:sequential-loops}

\section{In-Place Updates}
\label{sec:in-place-updates}

\begin{lstlisting}
-- A least significant digit radix sort to test out `write`.
fun radix_sort_up(xs: [n]u32) : ([n]u32,[n]i32) =
  let is = iota(n) in
  loop (p:([n]u32,[n]i32) = (xs,is)) = for i < 32 do
    radix_sort_step_up(p,i)
  in p
\end{lstlisting}


\section{Modules}

\section{Benchmarking}
\label{sec:benchmarking}

Consider an implementation of dot product:

\lstinputlisting{src/dotprod.fut}

We earlier mentioned that, for small data sets, sequential execution
is likely to be much faster than parallel execution.  But how much
faster?  To answer this question, we need to perform some simple
\textit{benchmarking}.  First, let us compile \texttt{dotprod.fut} to
two different executables, one for each compiler:

\begin{verbatim}
$ futhark-c dotprod.fut -o dotprod-c
$ futhark-opencl dotprod.fut -o dotprod-opencl
\end{verbatim}

One way to time execution is to use the standard \texttt{time(1)}
tool:

\begin{verbatim}
$ echo [2,2,3] [4,5,6] | time ./dotprod-c
36i32
0.00user 0.00system 0:00.00elapsed ...
$ echo [2,2,3] [4,5,6] | time ./dotprod-opencl
36i32
0.12user 0.00system 0:00.14elapsed ...
\end{verbatim}

It seems that \texttt{dotprod-c} executes in less than 10
milliseconds, while \texttt{dotprod-opencl} takes about 120
milliseconds.  However, this is not a truly useful comparison, as it
also measures time taken to read the input (for both executables), as
well as time taken to initialise the OpenCL driver (for
\texttt{dotprod-opencl}).  Recall that in a real application, the
Futhark program would be compiled as a \textit{library}, and the
startup cost paid just once, while the program may be invoked multiple
times.  A more precise run-time measurement, where parsing,
initialisation, and printing of results is not included, can be
performed using the \texttt{-t} command line option, which specifies a
file where the run-time (measured in microseconds) should be put:

\begin{verbatim}
$ echo [2,2,3] [4,5,6] | \
  ./dotprod-c -t /dev/stderr > /dev/null
0
\end{verbatim}

In this case, I ask for the runtime to be printed to the screen, and
for the normal evaluation result to be thrown away.  Apparently it
takes less than one microsecond to compute the dot product of two
three-element vectors on a CPU (this is not very surprising).  On an
AMD W8100 GPU:

\begin{verbatim}
$ echo [2,2,3] [4,5,6] | \
  ./dotprod-opencl -t /dev/stderr > /dev/null
575
\end{verbatim}

Almost half a millisecond!  GPUs have fairly high launch cost, and so
are not particularly suited for this small problem.  We can use the
\texttt{futhark-dataset(1)} tool to generate random test data of a
desired size:

\begin{verbatim}
$ futhark-dataset -g [10000000]int -g [10000000]int > input
\end{verbatim}

Two ten million element vectors should be enough work to amortise the
GPU startup cost:

\begin{verbatim}
$ cat input | ./dotprod-opencl -t /dev/stderr > /dev/null
2238
$ cat input | ./dotprod-c -t /dev/stderr > /dev/null
17078
\end{verbatim}

That's more like it - parallel execution is now more than seven times
faster than sequential execution.  This program is entirely
memory-bound - on a compute-bound algorithm we can expect much greater
speedups.

\section{When Things Go Wrong}

Futhark is a much younger and more raw language than you may be
accustomed to, and many common language features are missing.  It is
important to remember that Futhark is an \textit{on-going research
  project}, and you should not encounter the same predictability and
quality of error messages that you may be used to from more mature
languages.  In general, the limitations you will encounter will tend
to fall in three different categories:

\begin{description}
\item[Incidental] limitations are those languages features that are
  missing for no reason other than insufficient development resources.
  For example, Futhark does not support user-defined polymorphic
  functions, sum types, nontrivial type inference, or any kind of
  higher-order functions.  We know how to implement these, but simply
  have not gotten around to it yet.

\item[Essential] limitations touch upon fundamental restrictions in
  the target platform(s) for the Futhark compiler.  For example, GPUs
  do not permit dynamic memory allocation inside GPU code.  All memory
  must be pre-allocated before GPU programs are launched.  This means
  that the Futhark compiler must be able to pre-compute the size of
  all intermediate arrays (symbolically), or compilation will fail.

\item[Implementation] limitations are weaknesses in the Futhark
  compiler that could reasonably be solved.  Many implementation
  limitations, such as the inability to pre-compute some array sizes,
  or eliminate bounds checks inside parallel sections, will manifest
  themselves as essential limitations that could be worked around by a
  smarter compiler.
\end{description}

For example, consider this program:

\begin{lstlisting}
fun main(n: int): [][]int =
  map (fn i =>
         let a = iota i
         let b = iota (n-i)
         in concat a b)
  (iota n)
\end{lstlisting}

At the time of this writing, the \texttt{futhark-opencl} compiler will
fail with the not particularly illuminative error message
\texttt{Cannot allocate memory in kernel}.  The reason is that the
compiler is trying to compile the \texttt{map} to parallel code, which
involves pre-allocating memory for the \texttt{a} and \texttt{b}
array.  It is unable to do this, as the sizes of these two arrays
depend on values that are only known \textit{inside} the map, which is
too late.  There are various techniques the Futhark compiler could use
to estimate how much memory would be needed, but these have not yet
been implemented.

It is usually possible, sometimes with some pain, to come up with a
workaround.  We could rewrite the program as:

\begin{lstlisting}
fun main(n: int): [][]int =
  let big_iota = iota n
  in map (fn i =>
            let res = iota n
            let res[i:n] = big_iota[0:n-i]
            in res)
         (iota n)
\end{lstlisting}

This exploits the fact that the compiler does not generate allocations
for array slices or in-place updates.  The only allocation is of the
initial \texttt{res}, the size of which can be computed before
entering the \texttt{map}.

\chapter{Algebraic Properties of SOACs}
\label{chap:soac-algebra}

\begin{enumerate}
\item general reasoning principles
\item assumptions
\item fusion rules
\item list homomorphism theorem
\item let the compiler do the fusion (how to reason)
\end{enumerate}

\chapter{Parallel Cost Models}
\begin{enumerate}
\item motivation
\item memory vs compute bound
\item nested parallism and flattening
\item work and depth
\item Futhark specifics and limitations
\end{enumerate}

\part{Parallel Algorithms}

\chapter{Parallel Algorithms}
In this chapter, we will present a number of parallel algorithms for
solving a number of problems. We will make use effective use of the
SOAC parallel operators, in particular, it turns out that the
\soac{scan} operator is critical for obtaining parallel algorithms. In
fact, we shall first develop the notion of a \emph{segmented scan}
operation, which, as we shall see, can be implemented using Futhark's \soac{scan}
operator, and which in its own right is essential to many of the later
algorithms.

\section{Segmented Scan}

\lstinputlisting[firstline=7]{src/sgm_scan.fut}

\begin{enumerate}
\item segmented scan
\item radix sort
\lstinputlisting[firstline=18]{src/radix_sort.fut}
\item pseudo random numbers and sobol
\item trees
\item graphs
\item longest streak
\item segmented replication
\item histograms
\item parenthesis matching
\end{enumerate}

\chapter{Bigger Applications}
\begin{enumerate}
\item monte carlo
\item learning with stochastic gradient descent
\item stencils
\item convolutions
\end{enumerate}

\chapter{Interoperability}
\label{chap:interoperability}

\begin{enumerate}
\item python and c
\item examples: mandelbrot, life, cam, nbody
\end{enumerate}

\bibliographystyle{plain}
\bibliography{bib}

\appendix

\part{Appendices}

\chapter{Tool References}
\begin{enumerate}
\item futhark-c, futhark-opencl
\item measuring runtimes, debugging
\end{enumerate}

\end{document}

%%% Local Variables:
%%% mode: latex
%%% TeX-master: t
%%% End:
