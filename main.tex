\documentclass[11pt]{book}

\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{url}
\usepackage{amssymb,amsthm}
\usepackage[utf8]{inputenc}
%\usepackage{inconsolata}
\usepackage[scaled=.8]{sourcecodepro}

\input{futhark.tex}

\usepackage{color}
\definecolor{eclipseBlue}{RGB}{42,0.0,255}
\newcommand{\soac}[1]{\texttt{\color{eclipseBlue}#1}}

\title{\bf Parallel Programming in Futhark}
\author{HIPERFIT \\ Department of Computer Science \\ University of Copenhagen (DIKU)}
\date{\today}

\begin{document}
\frontmatter
\maketitle
\chapter{Preface}

This book is written with the aim of providinf an introduction to
parallel functional programming and algorithms with the focus on using
the Futhark language as the development language.

Futhark is currently being actively developed but it is mature enough
that simple algorithms---and also quite a few non-trivial ones---can
be written in the language and executed in parallel on real parallel
GPU hardware.

The book is Open Source, maintained on github, and distributed under
an XXX license. We will appreciate pull-requests for fixing any kinds
of typos and errors in the text and in the enclosed
programs. Regarding errors in the programs, most of the programs
written in the book can be run and tested against an expected result;
this execution and test can be done by typing \texttt{make} in the
\texttt{src/} directory of the repository.

\section*{Acknowledgments}
This work has been partially supported by the Danish Strategic Research
Council, Program Committee for Strategic Growth Technologies, for the
research center HIPERFIT: Functional High Performance Computing for
Financial Information Technology (\url{hiperfit.dk}) under contract number
10-092299.

\section*{Contributions}
Here we list contributions by non-HIPERFIT contributors.


\tableofcontents
\mainmatter
\part{Parallel Functional Programming}
\chapter{Introduction}
In 1965, Gordon E.\ Moore predicted a doubling every year in the number of
components in an integrated circuit \cite{moore1965}. He revised the prediction in 1975
to a doubling every two year \cite{moore1975} and later revisions suggest a slight
decrease in the growth rate, while the growth rate, here 50 years after Moore's first prediction, is not seriously
predicted to fade out in the next decade. In the first many years, the
increase in components per chip area, as predicted by ``Moore's law'', had a direct influence on
processor speed. The personal computer was getting popular and
software providers were happy beneficials of the so-called ``free
lunch'', which made programs running on single Central Processing
Units (CPUs) double in speed whenever new processors hit the market.

The days of the ``free lunches'' for sequentially written programs is
over. The physical speed limit for sequential processing unit has
pretty much been reached. Increases in processor clock frequency
introduces heat problems that are difficult to deal with and chip
providers have instead turned their focus on providing multiple cores in the
same chip. Thus, for programs to run faster on ever new
architectures, programs will have to make use of algorithms and data
structures that benefit from simultaneous, that is \emph{parallel},
execution on multiple cores. Newer architectures, such as Graphical
Processing Unis (GPUs), host a high number of cores that are designed
for parallel processing and over the coming decade, we will see a
drastic increase in the number of cores hosted in each chip.

In this book we distinguish between the notions of parallelism and
concurrency. By \emph{concurrency}, we refer to programming language
controls for coordinating work done by multiple virtual
processes. Such processes may in principle run on the same physical
processor (using for instance time sharing) or they may run on
multiple processors. Controlling the communication and dependencies
between multiple processes turns out to be immensely difficult and
programmers need to deal with problems such as unforseen
non-determinism and dead-locks, collectively named \emph{race
  conditions}, issues that emerges when two or more processes (and
their interaction with an external environment) interleave. By
\emph{parallelism}, on the other hand, we simply refer to the notion
of speeding up a program by making it run on multiple
processors. Given a program, we can analyze the program to discover
dependencies between units of computation and as such, the program
contains all the information there is to know about to which degree
the program can be executed in parallel. We emphasize here the notion
that a parallel program should result in the same output given an
input no-matter how many processors are used for executing the
program. On the other hand, we hope that running the program in
parallel with multiple processors will execute faster than if only one
processor is used. As we shall see, making predictable models for
determining whether a given program will run efficiently on a parallel
machine can be difficult, in particular in cases where the program is
inhomogenerously parallel at several levels, simultaneously.

Parallelism can be divided into the notions of \emph{task
  parallelism}, which emphasizes the concept of executing multiple
independent tasks in parallel, and \emph{data parallelism}, which
focuses on executing the same program on a number of different data
objects in parallel. At the hardware side, Multiple Instruction
Multiple Data (MIMD) processor designs directly allow for different
tasks to be executed in parallel. For such designs, each processor is
quite complex and in terms of fitting most processors on a single
chip, so as to increase overall throughput, vendors have had better
success by simplifying the chip designs in such a way that a compute
unit executes single instructions on multiple data (SIMD). Such
processor designs have turned out to be useful for a large number of
application domains, including graphics processing, machine learning,
image analysis, financial algorithms, and many more. In particular,
for graphics processing, chip designers have since the 1970'es
developed the concept of graphics processing units (GPUs), which, in
the later years, have turned into ``general purpose'' graphics
processing units (GPGPUs).

The notions of parallel processing and parallel programming are not
new. Concepts in these areas have emerged over a period of more than three
decades and today the notion of parallism appears in many
disguises. For example, the internet as we know it can be understood
as a giant parallel processing unit and whenever some user is browsing
and searching the internet, a large number of processing units are
working in parallel to provide the user with the best information
available on the topic. At all levels, software engineers need to know
how to exploit the ever increasing amount of computational resources.


\begin{enumerate}
\item Moores law, CPUs, GPUs, other parallel architectures (OK)
\item Concurrency vs parallelism (OK)
\item Task parallelism, data parallism, simd, mimd (OK)
\item Low-level languages vs high-level language approaches
\item Cost models for machines and for languages - problems with the RAM model, problems with the PRAM model, Work and Span.
\end{enumerate}

See \cite{finpar}.

\chapter{The Futhark Language}

Futhark is a pure functional data-parallel array language.  Is is both
syntactically and conceptually similar to established functional
languages, such as Haskell and Standard ML.  In contrast to these
languages, Futhark focuses less on expressivity and elaborate type
systems, but more on compilation to high-performance parallel code.
Futhark comes with language constructs for performing bulk operations
on arrays, called \textit{Second-Order Array Combinators} (SOACs),
that mirror the higher order functions found in conventional
functional languages: \texttt{map}, \texttt{reduce}, \texttt{filter},
and so forth.  In Futhark, SOACs are not merely library functions, but
built-in language features with parallel semantics, and which will
typically be compiled to parallel code.

The primary idea behind Futhark is to design a language that has
enough expressive power to conveniently express complex programs, yet
is also amenable to aggressive optimisation and parallelisation.  The
tension is that as the expressive power of a language grows, the
difficulty of efficient compilation rises likewise.  For example,
Futhark supports nested parallelism, despite the complexities of
efficiently mapping it to the flat parallelism supported by hardware,
as many algorithms are awkward to write with just flat parallelism.
On the other hand, we do not support non-regular arrays, as they
complicate size analysis a great deal.  The fact that Futhark is
purely functional is intended to give an optimising compiler more
leeway in rearranging the code and performing high-level
optimisations.

Programming in Futhark feels similar to programming in other
functional languages.  If you know Haskell or Standard ML, you will
likely be able to read and modify most Futhark code.  For example,
this program computes the dot product $\Sigma_{i} x_{i}\cdot{}y_{i}$
of two vectors of integers:

\noindent
\begin{minipage}{\textwidth}
\lstinputlisting{src/dotprod.fut}
\end{minipage}

In Futhark, the notation for an array of element type $t$ is
\texttt{[]$t$}.  The program declares a function called \texttt{main}
that takes two arguments, both integer arrays, and returns an integer.
The program first computes the element-wise product of its
two arguments, resulting in an array of integers, then computes the
product of the elements in this new array.

If you put this program in a file \texttt{dotprod.fut}, then you can
compile it to a binary \texttt{dotprod} (or \texttt{dotprod.exe} on
Windows) by running:

\begin{verbatim}
$ futhark-c dotprod.fut
\end{verbatim}

A Futhark program compiled to an executable will read the arguments to
its \texttt{main} function from standard input, and will print the
result to standard output:

\begin{verbatim}
$ echo [2,2,3] [4,5,6] | ./dotprod
36i32
\end{verbatim}

In Futhark, an array literal is written with square brackets
surrounding a comma-separated sequence of elements.  Integer literals
can be suffixed with a specific type.  This is why \texttt{dotprod}
prints \texttt{36i32}, rather than just \texttt{36} - this makes it
clear that the result is a 32-bit integer.  Later we will see when
these suffixes can be useful.

The \texttt{futhark-c} compiler we used above translates a Futhark
program into sequential code running on the GPU.  This can be useful
for testing, and will work on most systems, even those without GPUs.
However, this wastes the main potential of Futhark: fast parallel
execution.  We can instead use the \texttt{futhark-opencl} compiler to
generate an executable that offloads execution via the OpenCL
framework.  In principle, this allows offloading to any kind of
device, but the \texttt{futhark-opencl} compilation pipelines makes
optimisation assumptions that are oriented towards contemporary GPUs.
Use of \texttt{futhark-opencl} is simple, assuming your system has a
working OpenCL setup:

\begin{verbatim}
$ futhark-opencl dotprod.fut
\end{verbatim}

\noindent
Execution is just as before:

\begin{verbatim}
$ echo [2,2,3] [4,5,6] | ./dotprod
36i32
\end{verbatim}

In this case, the workload is small enough that there is little
benefit in parallelising the execution.  In fact, it is likely that
the OpenCL startup overhead results in several orders of magnitude
slowdown, over sequential execution, for this tiny dataset.  See
Section~\ref{sec:benchmarking} for information on how to measure
execution times.

The ability to compile Futhark programs to executables is useful for
testing, but it should be noted that it is not how Futhark is intended
to be used in practice.  As a pure functional array language, Futhark
is not capable of input handling or user interfaces, and as such
cannot be used as a general-purpose language.  Futhark is intended to
be used for small, performance-sensitive parts of larger applications,
typically by compiling a Futhark program to a \textit{library} that
can be imported and used by applications written in conventional
languages.  While this usage is still work-in-progress, see
Chapter~\ref{chap:interoperability} for more information.

As befits their use for testing, compiled Futhark executables take a
range of command line options to manipulate their behaviour and print
debugging information.  These will be introduced as needed.

\section{Basic Language Features}

As a functional or \textit{value-oriented} language, Futhark can be
understood entirely by how values are constructed, and how expressions
transform one value to another.  As a statically typed language, all
values are classified by their \textit{type}.  The primitive types in
Futhark are the signed integer types \texttt{i8}, \texttt{i16},
\texttt{i32}, \texttt{i64}, the unsigned integer types \texttt{u8},
\texttt{u16}, \texttt{u32}, \texttt{u64}, the floating-point types
\texttt{f32}, \texttt{f64}, as well as \texttt{bool}.  Furthermore,
\texttt{int} is an alias for \texttt{i32}.  An \texttt{f32} is always
a single-precision float and a \texttt{f64} is a double-precision
float.

Numeric literals can be suffixed with their intended type.  For
example \texttt{42i8} is of type \texttt{i8}, and \texttt{1337e2f64}
is of type \texttt{f64}.  If no suffix is given, integer literals are
of type \texttt{i32}, and decimal literals are of type \texttt{f64}.
Boolean literals are written as \texttt{true} and \texttt{false}.

All primitive values can be combined in tuples and arrays.  A tuple
value or type is written as a sequence of comma-separated values or
types enclosed in parentheses.  For example, \texttt{(0, 1)} is a
tuple value of type \texttt{(i32,i32)}.  The elements of a tuple need
not have the same type -- the value \texttt{(False, 1, 2.0)} is of
type \texttt{(bool, i32, f64)}.  A tuple element can also be another
tuple, as in \texttt{((1,2),(3,4))}, which is of type
\texttt{((i32,i32),(i32,i32))}.  A tuple cannot have just one element,
but empty tuples are permitted, although they are not very useful---these are
written \texttt{()} and are of type \texttt{()}.

An array value is written as a nonempty sequence of comma-separated
values enclosed in square brackets.  An array type is written as
\texttt{[]$t$}, where \texttt{$t$} is the element type of the array.
For example, an array of three integers could be written as
\texttt{[1,2,3]}, and has type \texttt{[]i32}.  An empty array is
written as \texttt{empty($t$)}, where \texttt{$t$} is the element
type.  Multi-dimensional arrays are supported in Futhark, but they
must be \textit{regular}.  This regularity requirement means that all
inner arrays must have the same shape.  For example, \texttt{[[1,2],
    [3,4]]} is a valid array of type \texttt{[][]i32}, but
\texttt{[[1,2], [3,4,5]]} is not.  We will return to the implications
of this restriction in later chapters.

\subsection{Simple Expressions}

The Futhark expression syntax is mostly conventional, and supports the
usual binary and unary operators, with few surprises.  This section
will not try to cover the entire Futhark expression language in
complete detail.  See the reference manual at
\url{http://futhark.readthedocs.io} for a comprehensive treatment.

Function calls are written using simple juxtaposition.  For example,
to apply the predefined \texttt{cos64} function (cosinus for
\texttt{f64} values) to a constant argument, we write:

\begin{lstlisting}
cos64 1.0
\end{lstlisting}

\noindent
See Section \ref{sec:function-declarations} for how to declare your
own functions.

A \texttt{let}-expression can be used to give a name to the result of
an expression:

\begin{lstlisting}
let z = x + y
in body
\end{lstlisting}

Futhark is eagerly evaluated (unlike Haskell), so the expression for
\texttt{z} will be fully evaluated before \texttt{body}.  The \texttt{in} keyword is optional when it precedes another
\texttt{let}.  Thus, instead of writing:

\begin{lstlisting}
let a = 0 in
let b = 1 in
let c = 2 in
a + b + c
\end{lstlisting}

\noindent
we can write

\begin{lstlisting}
let a = 0
let b = 1
let c = 2
in a + b + c
\end{lstlisting}

\noindent
The final \texttt{in} is still necessary.  In examples, we will often
skip the body of a \texttt{let} if it is not important.  A limited
amount of pattern matching is supported in \texttt{let}-bindings,
which permits tuple components to be extracted:

\begin{lstlisting}
let (x,y) = e      -- e must be of some type (t1,t2)
\end{lstlisting}

\noindent
This feature also demonstrates the Futhark line comment syntax---two dashes
followed by a space.  Block comments are not presently supported.

A two-way \texttt{if-then-else} is the only branching construct in
Futhark:

\begin{lstlisting}
if x < 0 then -x else x
\end{lstlisting}

Arrays are indexed using the common row-major notation, as in the expression
\texttt{a[i1, i2, i3, ...]}.  An indexing is said to be \textit{full} if
the number of given indices is equal to the dimensionality of the
array.  All array accesses are checked at runtime, and the program
will terminate abnormally if an invalid access is attempted.

Indexing binds very tightly.  For example, the expression
\texttt{a~b~[i]} means ``apply the function \texttt{a} to the
expression \texttt{b[i]}'', \textit{not} ``apply the function
\texttt{a} to the expressions \texttt{b} and \texttt{[i]}''.  When the
latter is desired, enclose the literal array with parentheses, as in
\texttt{a~b~([i])}.

Futhark also supports array \textit{slices}.  The expression
\texttt{a[i:j]} returns a slice of the array \texttt{a} from index
\texttt{i} to \texttt{j}, the latter inclusive and the latter
exclusive.  Slicing of multiple dimensions can be done by separating
with commas, and may be intermixed freely with indexing.  It is an
error if \texttt{j < n}.

\subsection{Function Definitions}
\label{sec:function-declarations}

A Futhark program consists of a sequence of \textit{function
  definitions}, of the following form:

\begin{lstlisting}
fun name params... : return_type = body
\end{lstlisting}

\noindent
A function must declare both its return type and the types of all its
parameters.  All functions (except for inline anonymous functions, as
we will get to later) are defined globally.  Futhark does not use type
inference.  As a concrete example, here is the recursive definition of
the factorial function in Futhark:

\begin{lstlisting}
fun fact(n: int): int =
  if n == 0 then 1
            else n * fact(n-1)
\end{lstlisting}

\noindent
Indentation has no syntactical significance in Futhark, but is
recommended for readability.

\section{Array Operations}

Futhark provides various combinators for performing bulk
transformations of arrays.  Judicious use of these combinators is key
to getting good performance.  There are two overall categories:
\textit{first-order array combinators}, like \texttt{concat}, that
always perform the same operation, and \textit{second-order array
  combinators} (\textit{SOAC}s), like \texttt{map}, that take a
\textit{functional argument} indicating the operation to perform.
SOACs are absolutely crucial to Futhark programming.  While they are
designed to resemble higher-order functions that may be familiar from
functional languages, they have implicitly parallel semantics, and
some restrictions to preserve those semantics.

The simplest SOAC is probably \texttt{map}.  It takes two arguments: a
function and an array.  The function argument can be a function name,
or an anonymous function using \texttt{fn} syntax.  The function is
called for every element of the input array, and an array of the result is
returned.  For example:

\begin{lstlisting}
map (fn x => x + 2) [1,2,3] == [3,4,5]
\end{lstlisting}

Functions defined using \texttt{fn} syntax need not define their
parameter- or return types, but you are free to do so to increase
readability:

\begin{lstlisting}
map (fn (x:int): int => x + 2) [1,2,3]
\end{lstlisting}

The functional argument can also be an operator, which must be
enclosed in parentheses:

\begin{lstlisting}
map (!) [True, False, True] == [False, True, False]
\end{lstlisting}

Currying for operators is also supported using a syntax taken from
Haskell:

\begin{lstlisting}
map (2-) [1,2,3] == [-1,0,1]
\end{lstlisting}

While \texttt{map} accepts only a single array argument, there is a
variation called \texttt{zipWith}, that takes any nonzero number of
array arguments, and requires a function with the same number of
parameters.  For example, we can perform an element-wise sum of two
arrays:

\begin{lstlisting}
zipWith (+) [1,2,3] [4,5,6] == [5,7,9]
\end{lstlisting}

While \texttt{map} is an array transformer, the \texttt{reduce} SOAC
is an array aggregator: it uses some function of type \texttt{t -> t
  -> t} to combine the elements of an array of type \texttt{[]t} to a
value of type \texttt{t}.  In order to do this in parallel, the
function must be \textit{associative} and have a \textit{neutral
  element} (i.e, form a monoid).

\begin{itemize}
\item A function $f$ is associative if $f(x,f(y,z)) = f(f(x,y),z)$ for
  all $x,y,z$.
\item A function $f$ has a neutral element $e$ if
  $f(x,e) = f(e,x) = x$ for all $x$.
\end{itemize}

Many common mathematical operators fulfill these laws, e.g addition:
$(x+y)+z=x+(y+z)$ and $x+0=0+x=0$.  But others, like subtraction, do
not.  In Futhark, we can use the addition operator and its neutral
element to compute the sum of an array of integers:

\begin{lstlisting}
reduce (+) 0 [1,2,3] == 6
\end{lstlisting}

It turns out that combining \texttt{map} and \texttt{reduce} is both
powerful and has remarkable optimisation properties, as we will
discuss in chapter~\ref{chap:soac-algebra}.  Many Futhark programs are
primarly \texttt{map}-\texttt{reduce} compositions.  For example, we
can define a function to compute the dot product of two vectors of
integers:

\begin{lstlisting}
fun dotProd (xs: []int) (ys: []int): []int =
  reduce (+) 0 (zipWith (+) xs ys)
\end{lstlisting}

A close cousin of \texttt{reduce} is \texttt{scan}, often called
\textit{generalised prefix sum}.  Where \texttt{reduce} produces just
one result, \texttt{scan} produces one result for every prefix of the
input array.  This is perhaps best understood with an example:

\begin{lstlisting}
scan (+) 0 [1,2,3] == [0+1, 0+1+2, 0+1+2+3] == [1, 3, 6]
\end{lstlisting}

Intuitively, the result of \texttt{scan} is an array of calling
\texttt{reduce} on increasing prefixes of the input array.  The last
element of the returned array is equivalent to the result of calling
\texttt{reduce} Like with \texttt{reduce}, the operator given to
\texttt{scan} must be associative and have a neutral element.

There are two main ways to compute scans: \textit{exclusive} and
\textit{inclusive}.  The difference is that the empty prefix is
considered in an exclusive scan, but not in an inclusive scan.
Computing the exclusive $+$-scan of \texttt{[1,2,3]} thus gives
\texttt{[0,1,3,6]}, while the inclusive $+$-scan is \texttt{[1,3,6]}.
The \texttt{scan} SOAC in Futhark is inclusive, but it is of course
easy to generate a corresponding exclusive scan simply by prepending
the neutral element.

While the idea behind \texttt{reduce} is probably familiar,
\texttt{scan} is a little more esoteric, and mostly has applications
for handling algorithms that do not seem parallel at first glance.
Several examples are discussed in
Chapter~\ref{chap:parallel-algorithms}.

\section{Sequential Loops}
\label{sec:sequential-loops}

Futhark has built-in syntax for expressing certain tail-recursive
functions.  Consider the following tail-recursive formulation of a
function for computing the Fibonacci numbers

\begin{lstlisting}
fun fib(n: int): int = fibhelper(1,1,n)
fun fibhelper(x: int, y: int, n: int): int =
  if n == 1 then x else fibhelper(y, x+y, n-1)
\end{lstlisting}

We can rewrite this using \texttt{loop} syntax:

\begin{lstlisting}
fun fib(n: int): int =
  loop ((x, y) = (1,1)) = for i < n do
                            (y, x+y)
  in x
\end{lstlisting}

The semantics of this is precisely as in the tail-recursive function
formulation.  In general, a loop:

\begin{lstlisting}
loop (pat = initial) = for i < bound do loopbody
in body
\end{lstlisting}

Has the following the semantics:

\begin{enumerate}
\item Bind \texttt{pat} to the initial values given in
  \texttt{initial}.
\item While \texttt{i < bound}, evaluate \texttt{loopbody}, rebinding
  \texttt{pat} to be the value returned by the body.  At the end of
  each iteration, increment \texttt{i} by one.
\item Evaluate \texttt{body} with \texttt{pat} bound to its final
  value.
\end{enumerate}

Semantically, a \texttt{loop} expression is completely equivalent to a
call to its corresponding tail-recursive function.

For example, denoting by \texttt{t} the type of \texttt{x}, this
loop:

\begin{lstlisting}
loop (x = a) =
  for i < n do
    g(x)
  in body
\end{lstlisting}

has the semantics of a call to this tail-recursive function:

\begin{lstlisting}
fun f(i: int, n: int, x: t): t =
  if i >= n then x
     else f(i+1, n, g(x))

-- the call
let x = f(i, n, a)
in body
\end{lstlisting}

The purpose of \texttt{loop} is partly to render some sequential
computations slightly more convenient, but primarily to express
certain very specific forms of recursive functions, specifically those
with a fixed iteration count.  This property is used for analysis and
optimisation by the Futhark compiler.  In contrast to most functional
languages, Futhark does \textit{not} perform tail call optimisation.
Furthermore, recursive functions may not work inside parallel
sections, depending on the targeted backend.  It is therefore strongly
recommended to use \texttt{loop} syntax instead of recursion.

Apart from the \texttt{i < n} form, which loops from zero, Futhark
also supports the \texttt{v <= i < n} form which starts at \texttt{v}.
We can also invert the order of iteration by writing \texttt{n > i} or
\texttt{n > i >= v}, which loops down from the upper bound to the
lower.  Due to parser limitations, most non-atomic expressions will
have to be parenthesised when used as the left-hand bound.

Apart from \texttt{for}-loops, Futhark also supports \texttt{while}
loops.  These do not provide as much information to the compiler, but
can be used for convergence loops, where the number of iterations
cannot be predicted in advance.  For example, the following program
doubles a given number until it exceeds a given threshold value::

\begin{lstlisting}
fun main(x: int, bound: int): int =
  loop (x) = while x < bound do x * 2
  in x
\end{lstlisting}

In all respects other than termination criteria, \texttt{while}-loops
behave identically to \texttt{for}-loops.

For brevity, the initial value expression can be elided, in which case
an expression equivalent to the pattern is implied.  This is easier to
understand with an example.  The loop

\begin{lstlisting}
fun fib(n: int): int =
  let x = 1
  let y = 1
  loop ((x, y) = (x, y)) = for i < n do (y, x+y)
  in x
\end{lstlisting}

can also be written

\begin{lstlisting}
fun fib(n: int): int =
  let x = 1
  let y = 1
  loop ((x, y)) = for i < n do (y, x+y)
  in x
\end{lstlisting}

This can sometimes make imperative code look more natural.

\section{In-Place Updates}
\label{sec:in-place-updates}

\begin{lstlisting}
-- A least significant digit radix sort to test out `write`.
fun radix_sort_up(xs: [n]u32) : ([n]u32,[n]i32) =
  let is = iota(n) in
  loop (p:([n]u32,[n]i32) = (xs,is)) = for i < 32 do
    radix_sort_step_up(p,i)
  in p
\end{lstlisting}


\section{Modules}

\section{Benchmarking}
\label{sec:benchmarking}

Consider an implementation of dot product:

\lstinputlisting{src/dotprod.fut}

We earlier mentioned that, for small data sets, sequential execution
is likely to be much faster than parallel execution.  But how much
faster?  To answer this question, we need to perform some simple
\textit{benchmarking}.  First, let us compile \texttt{dotprod.fut} to
two different executables, one for each compiler:

\begin{verbatim}
$ futhark-c dotprod.fut -o dotprod-c
$ futhark-opencl dotprod.fut -o dotprod-opencl
\end{verbatim}

One way to time execution is to use the standard \texttt{time(1)}
tool:

\begin{verbatim}
$ echo [2,2,3] [4,5,6] | time ./dotprod-c
36i32
0.00user 0.00system 0:00.00elapsed ...
$ echo [2,2,3] [4,5,6] | time ./dotprod-opencl
36i32
0.12user 0.00system 0:00.14elapsed ...
\end{verbatim}

It seems that \texttt{dotprod-c} executes in less than 10
milliseconds, while \texttt{dotprod-opencl} takes about 120
milliseconds.  However, this is not a truly useful comparison, as it
also measures time taken to read the input (for both executables), as
well as time taken to initialise the OpenCL driver (for
\texttt{dotprod-opencl}).  Recall that in a real application, the
Futhark program would be compiled as a \textit{library}, and the
startup cost paid just once, while the program may be invoked multiple
times.  A more precise run-time measurement, where parsing,
initialisation, and printing of results is not included, can be
performed using the \texttt{-t} command line option, which specifies a
file where the run-time (measured in microseconds) should be put:

\begin{verbatim}
$ echo [2,2,3] [4,5,6] | \
  ./dotprod-c -t /dev/stderr > /dev/null
0
\end{verbatim}

In this case, I ask for the runtime to be printed to the screen, and
for the normal evaluation result to be thrown away.  Apparently it
takes less than one microsecond to compute the dot product of two
three-element vectors on a CPU (this is not very surprising).  On an
AMD W8100 GPU:

\begin{verbatim}
$ echo [2,2,3] [4,5,6] | \
  ./dotprod-opencl -t /dev/stderr > /dev/null
575
\end{verbatim}

Almost half a millisecond!  GPUs have fairly high launch cost, and so
are not particularly suited for this small problem.  We can use the
\texttt{futhark-dataset(1)} tool to generate random test data of a
desired size:

\begin{verbatim}
$ futhark-dataset -g [10000000]int -g [10000000]int > input
\end{verbatim}

Two ten million element vectors should be enough work to amortise the
GPU startup cost:

\begin{verbatim}
$ cat input | ./dotprod-opencl -t /dev/stderr > /dev/null
2238
$ cat input | ./dotprod-c -t /dev/stderr > /dev/null
17078
\end{verbatim}

That's more like it - parallel execution is now more than seven times
faster than sequential execution.  This program is entirely
memory-bound - on a compute-bound algorithm we can expect much greater
speedups.

\section{When Things Go Wrong}

Futhark is a much younger and more raw language than you may be
accustomed to, and many common language features are missing.  It is
important to remember that Futhark is an \textit{on-going research
  project}, and you should not encounter the same predictability and
quality of error messages that you may be used to from more mature
languages.  In general, the limitations you will encounter will tend
to fall in three different categories:

\begin{description}
\item[Incidental] limitations are those languages features that are
  missing for no reason other than insufficient development resources.
  For example, Futhark does not support user-defined polymorphic
  functions, sum types, nontrivial type inference, or any kind of
  higher-order functions.  We know how to implement these, but simply
  have not gotten around to it yet.

\item[Essential] limitations touch upon fundamental restrictions in
  the target platform(s) for the Futhark compiler.  For example, GPUs
  do not permit dynamic memory allocation inside GPU code.  All memory
  must be pre-allocated before GPU programs are launched.  This means
  that the Futhark compiler must be able to pre-compute the size of
  all intermediate arrays (symbolically), or compilation will fail.

\item[Implementation] limitations are weaknesses in the Futhark
  compiler that could reasonably be solved.  Many implementation
  limitations, such as the inability to pre-compute some array sizes,
  or eliminate bounds checks inside parallel sections, will manifest
  themselves as essential limitations that could be worked around by a
  smarter compiler.
\end{description}

For example, consider this program:

\begin{lstlisting}
fun main(n: int): [][]int =
  map (fn i =>
         let a = iota i
         let b = iota (n-i)
         in concat a b)
  (iota n)
\end{lstlisting}

At the time of this writing, the \texttt{futhark-opencl} compiler will
fail with the not particularly illuminative error message
\texttt{Cannot allocate memory in kernel}.  The reason is that the
compiler is trying to compile the \texttt{map} to parallel code, which
involves pre-allocating memory for the \texttt{a} and \texttt{b}
array.  It is unable to do this, as the sizes of these two arrays
depend on values that are only known \textit{inside} the map, which is
too late.  There are various techniques the Futhark compiler could use
to estimate how much memory would be needed, but these have not yet
been implemented.

It is usually possible, sometimes with some pain, to come up with a
workaround.  We could rewrite the program as:

\begin{lstlisting}
fun main(n: int): [][]int =
  let big_iota = iota n
  in map (fn i =>
            let res = iota n
            let res[i:n] = big_iota[0:n-i]
            in res)
         (iota n)
\end{lstlisting}

This exploits the fact that the compiler does not generate allocations
for array slices or in-place updates.  The only allocation is of the
initial \texttt{res}, the size of which can be computed before
entering the \texttt{map}.

\chapter{Algebraic Properties of SOACs}
\label{chap:soac-algebra}

\begin{enumerate}
\item general reasoning principles
\item assumptions
\item fusion rules
\item list homomorphism theorem
\item let the compiler do the fusion (how to reason)
\end{enumerate}

\chapter{Parallel Cost Models}
\begin{enumerate}
\item motivation
\item memory vs compute bound
\item nested parallism and flattening
\item work and depth
\item Futhark specifics and limitations
\end{enumerate}

\part{Parallel Algorithms}

\chapter{Parallel Algorithms}
\label{chap:parallel-algorithms}

In this chapter, we will present a number of parallel algorithms for
solving a number of problems. We will make use effective use of the
SOAC parallel operators, in particular, it turns out that the
\soac{scan} operator is critical for obtaining parallel algorithms. In
fact, we shall first develop the notion of a \emph{segmented scan}
operation, which, as we shall see, can be implemented using Futhark's \soac{scan}
operator, and which in its own right is essential to many of the later
algorithms.

\section{Segmented Scan}

\lstinputlisting[firstline=7]{src/sgm_scan.fut}

\begin{enumerate}
\item segmented scan
\item radix sort
\lstinputlisting[firstline=18]{src/radix_sort.fut}
\item pseudo random numbers and sobol
\item trees
\item graphs
\item longest streak
\item segmented replication
\item histograms
\item parenthesis matching
\end{enumerate}

\chapter{Bigger Applications}
\begin{enumerate}
\item monte carlo
\item learning with stochastic gradient descent
\item stencils
\item convolutions
\end{enumerate}

\chapter{Interoperability}
\label{chap:interoperability}

\begin{enumerate}
\item python and c
\item examples: mandelbrot, life, cam, nbody
\end{enumerate}

\bibliographystyle{plain}
\bibliography{bib}

\appendix

\part{Appendices}

\chapter{Tool References}
\begin{enumerate}
\item futhark-c, futhark-opencl
\item measuring runtimes, debugging
\end{enumerate}

\end{document}

%%% Local Variables:
%%% mode: latex
%%% TeX-master: t
%%% End:
