\documentclass[11pt]{book}

\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{url}
\usepackage{amssymb,amsthm}
\usepackage[utf8]{inputenc}
%\usepackage{inconsolata}
\usepackage{sourcecodepro}

\input{futhark.tex}

\usepackage{color}
\definecolor{eclipseBlue}{RGB}{42,0.0,255}
\newcommand{\soac}[1]{\texttt{\color{eclipseBlue}#1}}

\title{\bf Parallel Programming in Futhark}
\author{HIPERFIT \\ Department of Computer Science \\ University of Copenhagen (DIKU)}
\date{\today}

\begin{document}
\frontmatter
\maketitle
\chapter{Preface}

These notes ...

\tableofcontents
\mainmatter
\part{Parallel Functional Programming}
\chapter{Introduction}

\begin{enumerate}
\item Moores law, CPUs, GPUs, other parallel architectures
\item Concurrency vs parallelism
\item Task parallelism, data parallism, simd, mimd
\item Low-level languages vs high-level language approaches
\end{enumerate}

See \cite{finpar}.

\chapter{The Futhark Language}

Futhark is a pure functional data-parallel array language.  Is is both
syntactically and conceptually similar to established functional
languages, such as Haskell or Standard ML.  In contrast to these
languages, Futhark focuses less on expressivity and elaborate type
systems, but more on compilation to high-performance parallel code.
Futhark comes with language constructs for performing bulk operations
on arrays, called \textit{Second-Order Array Combinators} (SOACs),
that mirror the higher order functions found in conventional
functional languages: \texttt{map}, \texttt{reduce}, \texttt{filter},
and so forth.  In Futhark, SOACs are not merely library functions, but
built-in language features with parallel semantics, and which will
typically be compiled to parallel code.

Programming in Futhark feels similar to programming in other
functional languages.  If you know Haskell or Standard ML, you will
likely be able to read and modify most Futhark code.  For example,
this program computes the dot product $\Sigma_{i} x_{i}\cdot{}y_{i}$
of two vectors of integers:

\lstinputlisting{src/dotprod.fut}

In Futhark, the notation for an array of element type $t$ is
\texttt{[]$t$}.  The program declares a function called \texttt{main}
that takes two arguments, both integer arrays, and returns an integer.
The program first computes computes the element-wise product of its
two arguments, resulting in an array of integers, then computes the
product of the elements in this new array.

If you put this program in a file \texttt{dotprod.fut}, then you can
compile it to a binary \texttt{dotprod} (or \texttt{dotprod.exe} on
Windows) by running:

\begin{verbatim}
$ futhark-c dotprod.fut
\end{verbatim}

A Futhark program compiled to an executable will read the arguments to
its \texttt{main} function from standard input, and will print the
result to standard output:

\begin{verbatim}
$ echo [2,2,3] [4,5,6] | ./dotprod
36i32
\end{verbatim}

In Futhark, an array literal is written with square brackets
surrounding a comma-separated sequence of elements.  Integer literals
can be suffixed with a specific type.  This is why \texttt{dotprod}
prints \texttt{36i32}, rather than just \texttt{36} - this makes it
clear that the result is a 32-bit integer.  Later we will see when
these suffixes can be useful.

The \texttt{futhark-c} compiler we used above translates a Futhark
program into sequential code running on the GPU.  This can be useful
for testing, and will work on most systems, even those without GPUs.
However, this wastes the main potential of Futhark: fast parallel
execution.  We can instead use the \texttt{futhark-opencl} compiler to
generate an executable that offloads execution via the OpenCL
framework.  In principle, this allows offloading to any kind of
device, but the \texttt{futhark-opencl} compilation pipelines makes
optimisation assumptions that are oriented towards contemporary GPUs.
Use of \texttt{futhark-opencl} is simple, assuming your system has a
working OpenCL setup:

\begin{verbatim}
$ futhark-opencl dotprod.fut
\end{verbatim}

And execution is just as before:

\begin{verbatim}
$ echo [2,2,3] [4,5,6] | ./dotprod
36i32
\end{verbatim}

In this case, the workload is small enough that there is little
benefit in parallelising the execution.  In fact, it is likely that
the OpenCL startup overhead results in several orders of magnitude
slowdown, over sequential execution, for this tiny dataset.  See
Section~\ref{sec:benchmarking} for information on how to measure
execution times.

The ability to compile Futhark programs to executables is useful for
testing, but it should be noted that it is not how Futhark is intended
to be used in practice.  As a pure functional array language, Futhark
is not capable of input handling or user interfaces, and as such
cannot be used as a general-purpose language.  Futhark is intended to
be used for small, performance-sensitive parts of larger applications,
typically by compiling a Futhark program to a \textit{library} that
can be imported and used by applications written in conventional
languages.  While this usage is still work-in-progress, see
Chapter~\ref{chap:interoperability} for more information.

As befits their use for testing, compiled Futhark executables take a
range of command line options to manipulate their behaviour and print
debugging information.  These will be introduced as neded.

\section{Core Language}

\section{In-Place Updates}

\begin{lstlisting}
-- A least significant digit radix sort to test out `write`.
fun radix_sort_up(xs: [n]u32) : ([n]u32,[n]i32) =
  let is = iota(n) in
  loop (p:([n]u32,[n]i32) = (xs,is)) = for i < 32 do
    radix_sort_step_up(p,i)
  in p
\end{lstlisting}


\section{Modules}

\section{Benchmarking}
\label{sec:benchmarking}

Consider an implementation of dot product:

\lstinputlisting{src/dotprod.fut}

We earlier mentioned that, for small data sets, sequential execution
is likely to be much faster than parallel execution.  But how much
faster?  To answer this question, we need to perform some simple
\textit{benchmarking}.  First, let us compile \texttt{dotprod.fut} to
two different executables, one for each compiler:

\begin{verbatim}
$ futhark-c dotprod.fut -o dotprod-c
$ futhark-opencl dotprod.fut -o dotprod-opencl
\end{verbatim}

One way to time execution is to use the standard \texttt{time(1)}
tool:

\begin{verbatim}
$ echo [2,2,3] [4,5,6] | time ./dotprod-c
36i32
0.00user 0.00system 0:00.00elapsed ...
$ echo [2,2,3] [4,5,6] | time ./dotprod-opencl
36i32
0.12user 0.00system 0:00.14elapsed ...
\end{verbatim}

It seems that \texttt{dotprod-c} executes in less than 10
milliseconds, while \texttt{dotprod-opencl} takes about 120
milliseconds.  However, this is not a truly useful comparison, as it
also measures time taken to read the input (for both executables), as
well as time taken to initialise the OpenCL driver (for
\texttt{dotprod-opencl}).  Recall that in a real application, the
Futhark program would be compiled as a \textit{library}, and the
startup cost paid just once, while the program may be invoked multiple
times.  A more precise run-time measurement, where parsing,
initialisation, and printing of results is not included, can be
performed using the \texttt{-t} command line option, which specifies a
file where the run-time (measured in microseconds) should be put:

\begin{verbatim}
$ echo [2,2,3] [4,5,6] | \
  ./dotprod-c -t /dev/stderr > /dev/null
0
\end{verbatim}

In this case, I ask for the runtime to be printed to the screen, and
for the normal evaluation result to be thrown away.  Apparently it
takes less than one microsecond to compute the dot product of two
three-element vectors on a CPU (this is not very surprising).  On an
AMD W8100 GPU:

\begin{verbatim}
$ echo [2,2,3] [4,5,6] | \
  ./dotprod-opencl -t /dev/stderr > /dev/null
575
\end{verbatim}

Almost half a millisecond!  GPUs have fairly high launch cost, and so
are not particularly suited for this small problem.  We can use the
\texttt{futhark-dataset(1)} tool to generate random test data of a
desired size:

\begin{verbatim}
$ futhark-dataset -g [10000000]int -g [10000000]int > input
\end{verbatim}

Two ten million element vectors should be enough work to amortise the
GPU startup cost:

\begin{verbatim}
$ cat input | ./dotprod-opencl -t /dev/stderr > /dev/null
2238
$ cat input | ./dotprod-c -t /dev/stderr > /dev/null
17078
\end{verbatim}

That's more like it - parallel execution is now more than seven times
faster than sequential execution.  This program is entirely
memory-bound - on a compute-bound algorithm we can expect much greater
speedups.

\section{When Things Go Wrong}

Futhark is a much younger and more raw language than you may be
accustomed to, and many common language features are missing.  It is
important to remember that Futhark is an \textit{on-going research
  project}, and you should not encounter the same predictability and
quality of error messages that you may be used to from more mature
languages.  In general, the limitations you will encounter will tend
to fall in three different categories:

\begin{description}
\item[Incidental] limitations are those languages features that are
  missing for no reason other than insufficient development resources.
  For example, Futhark does not support user-defined polymorphic
  functions, sum types, nontrivial type inference, or any kind of
  higher-order functions.  We know how to implement these, but simply
  have not gotten around to it yet.

\item[Essential] limitations touch upon fundamental restrictions in
  the target platform(s) for the Futhark compiler.  For example, GPUs
  do not permit dynamic memory allocation inside GPU code.  All memory
  must be pre-allocated before GPU programs are launched.  This means
  that the Futhark compiler must be able to pre-compute the size of
  all intermediate arrays (symbolically), or compilation will fail.

\item[Implementation] limitations are weaknesses in the Futhark
  compiler that could reasonably be solved.  Many implementation
  limitations, such as the inability to pre-compute some array sizes,
  or eliminate bounds checks inside parallel sections, will manifest
  themselves as essential limitations that could be worked around by a
  smarter compiler.
\end{description}

For example, consider this program:

\begin{lstlisting}
fun main(n: int): [][]int =
  map (fn i =>
         let a = iota i
         let b = iota (n-i)
         in concat a b)
  (iota n)
\end{lstlisting}

At the time of this writing, the \texttt{futhark-opencl} compiler will
fail with the not particularly illuminative error message
\texttt{Cannot allocate memory in kernel}.  The reason is that the
compiler is trying to compile the \texttt{map} to parallel code, which
involves pre-allocating memory for the \texttt{a} and \texttt{b}
array.  It is unable to do this, as the sizes of these two arrays
depend on values that are only known \textit{inside} the map, which is
too late.  There are various techniques the Futhark compiler could use
to estimate how much memory would be needed, but these have not yet
been implemented.

It is usually possible, sometimes with some pain, to come up with a
workaround.  We could rewrite the program as:

\begin{lstlisting}
fun main(n: int): [][]int =
  let big_iota = iota n
  in map (fn i =>
            let res = iota n
            let res[i:n] = big_iota[0:n-i]
            in res)
         (iota n)
\end{lstlisting}

This exploits the fact that the compiler does not generate allocations
for array slices or in-place updates.  The only allocation is of the
initial \texttt{res}, the size of which can be computed before
entering the \texttt{map}.

\chapter{Algebraic Properties of SOACs}
\begin{enumerate}
\item general reasoning principles
\item assumptions
\item fusion rules
\item list homomorphism theorem
\item let the compiler do the fusion (how to reason)
\end{enumerate}

\chapter{Parallel Cost Models}
\begin{enumerate}
\item motivation
\item memory vs compute bound
\item nested parallism and flattening
\item work and depth
\item Futhark specifics and limitations
\end{enumerate}

\part{Parallel Algorithms}

\chapter{Parallel Algorithms}
In this chapter, we will present a number of parallel algorithms for
solving a number of problems. We will make use effective use of the
SOAC parallel operators, in particular, it turns out that the
\soac{scan} operator is critical for obtaining parallel algorithms. In
fact, we shall first develop the notion of a \emph{segmented scan}
operation, which, as we shall see, can be implemented using Futhark's \soac{scan}
operator, and which in its own right is essential to many of the later
algorithms.

\section{Segmented Scan}

\lstinputlisting[firstline=7]{src/sgm_scan.fut}

\begin{enumerate}
\item segmented scan
\item radix sort
\lstinputlisting[firstline=18]{src/radix_sort.fut}
\item pseudo random numbers and sobol
\item trees
\item graphs
\item longest streak
\item segmented replication
\item histograms
\item parenthesis matching
\end{enumerate}

\chapter{Bigger Applications}
\begin{enumerate}
\item monte carlo
\item learning with stochastic gradient descent
\item stencils
\item convolutions
\end{enumerate}

\chapter{Interoperability}
\label{chap:interoperability}

\begin{enumerate}
\item python and c
\item examples: mandelbrot, life, cam, nbody
\end{enumerate}

\bibliographystyle{plain}
\bibliography{bib}

\appendix

\part{Appendices}

\chapter{Tool References}
\begin{enumerate}
\item futhark-c, futhark-opencl
\item measuring runtimes, debugging
\end{enumerate}

\end{document}

%%% Local Variables:
%%% mode: latex
%%% TeX-master: t
%%% End:
